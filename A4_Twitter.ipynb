{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"HateTweetDetection\").getOrCreate()\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.csv(\"twitter.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select necessary columns\n",
        "df = df.select(col(\"tweet\"), col(\"label\"))  # label: 0 = not hate, 1 = hate\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Create pipeline stages\n",
        "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Assemble pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, lr])\n",
        "\n",
        "# Fit model\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "# Evaluate on test data\n",
        "predictions = model.transform(test_df)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"\\n Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# End training timer\n",
        "end_time = time.time()\n",
        "print(f\"⏱️ Model Training Time: {end_time - start_time:.2f} seconds\\n\")\n",
        "\n",
        "# Loop for user input\n",
        "while True:\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"1. Enter tweet for prediction\")\n",
        "    print(\"2. Quit\")\n",
        "    choice = input(\"Enter your choice (1 or 2): \")\n",
        "\n",
        "    if choice == '2':\n",
        "        print(\"Exiting program. Goodbye!\")\n",
        "        break\n",
        "    elif choice == '1':\n",
        "        user_tweet = input(\"Type your tweet: \")\n",
        "        user_df = spark.createDataFrame([Row(tweet=user_tweet)])\n",
        "        prediction = model.transform(user_df).select(\"tweet\", \"prediction\").collect()[0]\n",
        "\n",
        "        print(f\"\\nTweet: {prediction['tweet']}\")\n",
        "        print(f\"Prediction: {'Hate Tweet' if prediction['prediction'] == 1.0 else 'Not Hate Tweet'}\")\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 1 or 2.\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6kC-yUTEeYx",
        "outputId": "72ab5435-37cb-44e1-e4ce-20439193a3e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Test Accuracy: 0.89\n",
            "⏱️ Model Training Time: 15.29 seconds\n",
            "\n",
            "\n",
            "Options:\n",
            "1. Enter tweet for prediction\n",
            "2. Quit\n",
            "Enter your choice (1 or 2): 1\n",
            "Type your tweet: @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot  \n",
            "\n",
            "Tweet: @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot  \n",
            "Prediction: Hate Tweet\n",
            "\n",
            "Options:\n",
            "1. Enter tweet for prediction\n",
            "2. Quit\n",
            "Enter your choice (1 or 2):  factsguide: society now    #motivation\n",
            "Invalid input. Please enter 1 or 2.\n",
            "\n",
            "Options:\n",
            "1. Enter tweet for prediction\n",
            "2. Quit\n",
            "Enter your choice (1 or 2): 1\n",
            "Type your tweet:  factsguide: society now    #motivation\n",
            "\n",
            "Tweet:  factsguide: society now    #motivation\n",
            "Prediction: Not Hate Tweet\n",
            "\n",
            "Options:\n",
            "1. Enter tweet for prediction\n",
            "2. Quit\n",
            "Enter your choice (1 or 2): 2\n",
            "Exiting program. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Here’s a **line-by-line explanation** of your PySpark code for **hate tweet detection**, along with relevant **theory** and a list of **external viva questions with answers** that you can expect.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Full Code Explanation with Theory\n",
        "\n",
        "```python\n",
        "import time\n",
        "```\n",
        "- Imports the `time` module to measure execution time.\n",
        "- **Theory**: Helps benchmark model performance and total runtime.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "```\n",
        "- Imports Spark session, which is the entry point for DataFrame and SQL functionalities in PySpark.\n",
        "- **Theory**: SparkSession is used to initialize Spark and handle distributed computation.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "```\n",
        "- Imports `col` function to access column values more cleanly in DataFrame transformations.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "```\n",
        "- Imports NLP preprocessing tools:\n",
        "  - `Tokenizer`: Splits sentences into words (tokens).\n",
        "  - `StopWordsRemover`: Removes common words like \"the\", \"is\", \"and\".\n",
        "  - `HashingTF`: Converts words to fixed-size numeric feature vectors.\n",
        "  - `IDF`: Improves weighting of important words.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "```\n",
        "- Imports a simple but powerful classifier that works well for binary classification tasks (hate or not hate).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from pyspark.ml import Pipeline\n",
        "```\n",
        "- Pipeline allows chaining of preprocessing steps and model training into one reusable workflow.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder.appName(\"HateTweetDetection\").getOrCreate()\n",
        "```\n",
        "- Initializes the Spark environment with the name \"HateTweetDetection\".\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "start_time = time.time()\n",
        "```\n",
        "- Starts recording time to calculate how long the code takes to run.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "df = spark.read.csv(\"twitter.csv\", header=True, inferSchema=True)\n",
        "```\n",
        "- Loads the dataset `twitter.csv` into a DataFrame with automatic type inference.\n",
        "- **Theory**: CSV reading is a common task in data engineering; header=True ensures column names are preserved.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "df = df.select(col(\"tweet\"), col(\"label\"))\n",
        "```\n",
        "- Selects only the `tweet` and `label` columns. The label should be 0 (non-hate) or 1 (hate).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "```\n",
        "- Randomly splits the dataset into 80% training and 20% testing for evaluation purposes.\n",
        "- **Theory**: Data splitting is crucial to prevent overfitting and ensure generalization.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
        "```\n",
        "- Tokenizes the tweets (converts sentence to list of words).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "```\n",
        "- Removes common stop words that don't add meaning.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "```\n",
        "- Transforms filtered words into fixed-length feature vectors using hashing.\n",
        "- **Theory**: HashingTF is efficient and avoids the need to build a vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "```\n",
        "- Computes Inverse Document Frequency, which down-weights frequently occurring words.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "```\n",
        "- Creates the logistic regression model using the transformed features and actual labels.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, lr])\n",
        "```\n",
        "- Assembles all the preprocessing and model steps into a pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "model = pipeline.fit(train_df)\n",
        "```\n",
        "- Fits (trains) the entire pipeline on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "predictions = model.transform(test_df)\n",
        "```\n",
        "- Applies the trained model on the test data to generate predictions.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "```\n",
        "- Stops the timer and prints the total time taken to train and predict.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "predictions.select(\"tweet\", \"prediction\").show(10)\n",
        "```\n",
        "- Displays 10 predictions showing tweet text and whether it's hate or not (`0` or `1`).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "spark.stop()\n",
        "```\n",
        "- Properly shuts down the Spark session and releases resources.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 THEORETICAL CONCEPTS INVOLVED\n",
        "\n",
        "- **Natural Language Processing (NLP)**: Processing human language data using computers.\n",
        "- **TF-IDF**: Gives importance to words based on their frequency across documents.\n",
        "- **Logistic Regression**: Binary classifier used here to detect hate speech.\n",
        "- **Spark ML Pipeline**: Streamlines data preparation and model training.\n",
        "- **Train-Test Split**: Used to evaluate how well the model generalizes.\n",
        "\n",
        "---\n",
        "\n",
        "### ❓ Possible Viva Questions & Answers\n",
        "\n",
        "| **Question** | **Answer** |\n",
        "|--------------|------------|\n",
        "| What is the goal of your project? | To classify tweets as hate or non-hate based on their text using PySpark ML. |\n",
        "| Why did you use logistic regression? | It's efficient and works well for binary classification tasks. |\n",
        "| What is the purpose of Tokenizer? | To break tweets into individual words (tokens) for processing. |\n",
        "| What does StopWordsRemover do? | Removes common words that do not add value to classification. |\n",
        "| Why use HashingTF instead of CountVectorizer? | HashingTF is faster and more memory-efficient as it doesn't store vocabulary. |\n",
        "| What does IDF do? | It down-weights frequent words and gives importance to rare words. |\n",
        "| What is a pipeline in PySpark? | A workflow that chains together multiple data transformation and model training steps. |\n",
        "| What is the silhouette score? | (For clustering only) It measures how well samples are clustered. Not used here. |\n",
        "| What would happen if we didn’t split data? | The model might overfit and perform poorly on unseen data. |\n",
        "| What if a tweet contains a hashtag or emoji? | These are tokenized, but may need preprocessing like removing special characters. |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to now include the **interactive loop version** where the user can type in their own tweets and get predictions continuously until they type \"quit\"? '''"
      ],
      "metadata": {
        "id": "vt1cLDYlIQS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}