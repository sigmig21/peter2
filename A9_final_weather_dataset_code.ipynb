{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jembBkBfN4m7","outputId":"2f0e54bc-19a5-4558-f68e-2de4131393f2","executionInfo":{"status":"ok","timestamp":1745287628493,"user_tz":-330,"elapsed":10446,"user":{"displayName":"GAURAV TARATE","userId":"11463835749028390172"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloaded: ./weather_data/2015/99495199999.csv\n","Downloaded: ./weather_data/2015/72429793812.csv\n","Failed to download https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2016/99495199999.csv. Status code: 404\n","Downloaded: ./weather_data/2016/72429793812.csv\n","Downloaded: ./weather_data/2017/99495199999.csv\n","Downloaded: ./weather_data/2017/72429793812.csv\n","Downloaded: ./weather_data/2018/99495199999.csv\n","Downloaded: ./weather_data/2018/72429793812.csv\n","Downloaded: ./weather_data/2019/99495199999.csv\n","Downloaded: ./weather_data/2019/72429793812.csv\n","Downloaded: ./weather_data/2020/99495199999.csv\n","Downloaded: ./weather_data/2020/72429793812.csv\n","Downloaded: ./weather_data/2021/99495199999.csv\n","Downloaded: ./weather_data/2021/72429793812.csv\n","Downloaded: ./weather_data/2022/99495199999.csv\n","Downloaded: ./weather_data/2022/72429793812.csv\n","Downloaded: ./weather_data/2023/99495199999.csv\n","Downloaded: ./weather_data/2023/72429793812.csv\n"]}],"source":["import requests\n","import os\n","\n","base_url_1 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/99495199999.csv\"\n","base_url_2 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/72429793812.csv\"\n","\n","years = range(2015, 2024)\n","\n","base_output_dir = \"./weather_data/\"\n","\n","for year in years:\n","    year_dir = os.path.join(base_output_dir, str(year))\n","    os.makedirs(year_dir, exist_ok=True)\n","\n","    for base_url, station_id in [(base_url_1, \"99495199999\"), (base_url_2, \"72429793812\")]:\n","        url = base_url.format(year)\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","\n","            file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n","            with open(file_path, \"wb\") as file:\n","                file.write(response.content)\n","            print(f\"Downloaded: {file_path}\")\n","        else:\n","            print(f\"Failed to download {url}. Status code: {response.status_code}\")"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","base_input_dir = \"/content/weather_data\"\n","base_output_dir = \"/content/cleaned_weather_data\"\n","\n","invalid_values = {\n","    \"MXSPD\": 999.9,\n","    \"MAX\": 9999.9,\n","}\n","\n","for year in range(2015, 2023):\n","    year_dir = os.path.join(base_input_dir, str(year))\n","\n","    if not os.path.exists(year_dir):\n","        print(f\"Year directory not found: {year_dir}\")\n","        continue\n","\n","    for station_id in [\"99495199999\", \"72429793812\"]:\n","        file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n","\n","        if not os.path.exists(file_path):\n","            print(f\"File not found: {file_path}\")\n","            continue\n","\n","        df = pd.read_csv(file_path)\n","\n","        for column, invalid_value in invalid_values.items():\n","            if column in df.columns:\n","                df = df[df[column] != invalid_value]\n","\n","        output_year_dir = os.path.join(base_output_dir, str(year))\n","        os.makedirs(output_year_dir, exist_ok=True)\n","\n","        cleaned_file_path = os.path.join(output_year_dir, f\"{station_id}.csv\")\n","        df.to_csv(cleaned_file_path, index=False)\n","        print(f\"Cleaned data saved to: {cleaned_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2vx41BxOSiP","outputId":"2711a83a-4f53-4568-f359-bda18d957c23","executionInfo":{"status":"ok","timestamp":1745287635417,"user_tz":-330,"elapsed":600,"user":{"displayName":"GAURAV TARATE","userId":"11463835749028390172"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned data saved to: /content/cleaned_weather_data/2015/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2015/72429793812.csv\n","File not found: /content/weather_data/2016/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2016/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2017/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2017/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2018/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2018/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2019/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2019/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2020/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2020/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2021/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2021/72429793812.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2022/99495199999.csv\n","Cleaned data saved to: /content/cleaned_weather_data/2022/72429793812.csv\n"]}]},{"cell_type":"code","source":["import os\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","\n","spark = SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate()\n","\n","base_path = \"/content/cleaned_weather_data\"\n","\n","hottest_days = {}\n","\n","for year in range(2015, 2023):\n","    year_dir = os.path.join(base_path, str(year))\n","\n","    if not os.path.exists(year_dir):\n","        print(f\"Year directory not found: {year_dir}\")\n","        continue\n","\n","    for filename in os.listdir(year_dir):\n","        if filename.endswith('.csv'):\n","            file_path = os.path.join(year_dir, filename)\n","\n","            df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","            if df.rdd.isEmpty():\n","                print(f\"Skipping empty file: {filename}\")\n","                continue\n","\n","            if \"MAX\" not in df.columns:\n","                df = df.withColumn(\"MAX\", F.lit(None))\n","\n","            max_temp = df.agg(F.max(\"MAX\")).collect()[0][0]\n","\n","            if max_temp is not None:\n","                max_day = df.filter(df[\"MAX\"] == max_temp).orderBy(F.desc(\"DATE\")).first()\n","\n","                if max_day:\n","\n","                    if year not in hottest_days:\n","                        hottest_days[year] = (max_day.STATION, max_day.NAME, max_day.DATE, max_day.MAX)\n","\n","if hottest_days:\n","    hottest_days_list = [(year, *data) for year, data in hottest_days.items()]\n","    hottest_days_df = spark.createDataFrame(hottest_days_list, [\"YEAR\", \"STATION\", \"NAME\", \"DATE\", \"MAX\"])\n","    hottest_days_df.show()\n","else:\n","    print(\"No hottest days found across the datasets.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92qTugbsQP8A","outputId":"18e5ce73-7b62-4975-b95c-5b08bce4cb44","executionInfo":{"status":"ok","timestamp":1745287680031,"user_tz":-330,"elapsed":35024,"user":{"displayName":"GAURAV TARATE","userId":"11463835749028390172"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping empty file: 99495199999.csv\n","Skipping empty file: 99495199999.csv\n","+----+-----------+--------------------+----------+----+\n","|YEAR|    STATION|                NAME|      DATE| MAX|\n","+----+-----------+--------------------+----------+----+\n","|2015|99495199999|SEBASTIAN INLET S...|2015-07-28|90.0|\n","|2016|72429793812|CINCINNATI MUNICI...|2016-07-26|93.9|\n","|2017|99495199999|SEBASTIAN INLET S...|2017-05-24|88.3|\n","|2018|99495199999|SEBASTIAN INLET S...|2018-09-15|90.1|\n","|2019|99495199999|SEBASTIAN INLET S...|2019-09-06|91.6|\n","|2020|99495199999|SEBASTIAN INLET S...|2020-07-13|91.8|\n","|2021|72429793812|CINCINNATI MUNICI...|2021-08-25|95.0|\n","|2022|72429793812|CINCINNATI MUNICI...|2022-06-23|96.1|\n","+----+-----------+--------------------+----------+----+\n","\n"]}]},{"cell_type":"code","source":["import os\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","\n","spark = SparkSession.builder.appName(\"Coldest Day in March\").getOrCreate()\n","\n","base_path = \"/content/cleaned_weather_data\"\n","\n","march_data = []\n","\n","for year in range(2015, 2023):\n","    year_dir = os.path.join(base_path, str(year))\n","\n","    if not os.path.exists(year_dir):\n","        print(f\"Year directory not found: {year_dir}\")\n","        continue\n","\n","    for filename in os.listdir(year_dir):\n","        if filename.endswith('.csv'):\n","            file_path = os.path.join(year_dir, filename)\n","\n","            df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","\n","            if df.rdd.isEmpty():\n","                print(f\"Skipping empty file: {filename}\")\n","                continue\n","\n","            if \"DATE\" not in df.columns or \"MIN\" not in df.columns:\n","                print(f\"Skipping {filename} due to missing 'DATE' or 'MIN' column.\")\n","                continue\n","\n","            march_df = df.filter(df[\"DATE\"].contains('-03-'))\n","\n","            if march_df.rdd.isEmpty():\n","                continue\n","\n","            coldest_day = march_df.orderBy(F.asc(\"MIN\")).first()\n","\n","            if coldest_day is not None:\n","                march_data.append((coldest_day.STATION, coldest_day.NAME, coldest_day.DATE, coldest_day.MIN))\n","\n","if march_data:\n","    coldest_day_df = spark.createDataFrame(march_data, [\"STATION\", \"NAME\", \"DATE\", \"MIN\"])\n","\n","    overall_coldest_day = coldest_day_df.orderBy(F.asc(\"MIN\")).first()\n","\n","    if overall_coldest_day:\n","        overall_coldest_day_df = spark.createDataFrame([overall_coldest_day], [\"STATION\", \"NAME\", \"DATE\", \"MIN\"])\n","        print(\"\\nOverall coldest day in March:\")\n","        overall_coldest_day_df.show()\n","else:\n","    print(\"No March data found across the datasets.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GKPB6EORGqA","outputId":"6b71a408-22f7-4d1f-e45f-6629acc3d731","executionInfo":{"status":"ok","timestamp":1745287867215,"user_tz":-330,"elapsed":16445,"user":{"displayName":"GAURAV TARATE","userId":"11463835749028390172"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping empty file: 99495199999.csv\n","Skipping empty file: 99495199999.csv\n","\n","Overall coldest day in March:\n","+-----------+--------------------+----------+---+\n","|    STATION|                NAME|      DATE|MIN|\n","+-----------+--------------------+----------+---+\n","|72429793812|CINCINNATI MUNICI...|2015-03-06|3.2|\n","+-----------+--------------------+----------+---+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RJdp4EshRHcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''Step-wise Breakdown of the Code\n","1. Downloading Weather Data\n","\n","Purpose: Retrieve weather data for two specific stations (99495199999 and 72429793812) from the NOAA website for the years 2015 to 2023.\n","\n","Libraries Used:\n","\n","requests: For making HTTP requests to download CSV files.\n","os: For file system operations like creating directories and constructing file paths.\n","\n","\n","Process:\n","\n","Define base URLs for two weather stations using string formatting.\n","Iterate over years from 2015 to 2023.\n","For each year and station:\n","Construct the URL using the year.\n","Send an HTTP GET request using requests.get().\n","If the response status code is 200 (success), save the CSV content to a file in a directory structure (./weather_data/{year}/{station_id}.csv).\n","Print a success message with the file path or a failure message with the status code (e.g., 404 for not found).\n","\n","\n","\n","\n","Output Example:\n","\n","Successful download: Downloaded: ./weather_data/2015/99495199999.csv\n","Failed download: Failed to download .../2016/99495199999.csv. Status code: 404\n","\n","\n","\n","\n","2. Cleaning Weather Data\n","\n","Purpose: Clean the downloaded CSV files by removing rows with invalid or missing data in specific columns.\n","\n","Libraries Used:\n","\n","os: For file system operations.\n","pandas: For reading, manipulating, and saving CSV data as DataFrames.\n","\n","\n","Process:\n","\n","Define the input directory (/content/weather_data) and output directory (/content/cleaned_weather_data).\n","Specify invalid values for columns: MXSPD (999.9) and MAX (9999.9).\n","Iterate over years 2015 to 2022.\n","For each year and station:\n","Check if the year directory and CSV file exist; skip if not.\n","Load the CSV into a Pandas DataFrame using pd.read_csv().\n","Filter out rows where MXSPD or MAX equals the invalid values.\n","Save the cleaned DataFrame to a new CSV file in the output directory (/content/cleaned_weather_data/{year}/{station_id}.csv).\n","Print the path of the saved cleaned file.\n","\n","\n","\n","\n","Output Example:\n","\n","Cleaned data saved to: /content/cleaned_weather_data/2015/99495199999.csv\n","File not found: /content/weather_data/2016/99495199999.csv\n","\n","\n","\n","\n","3. Analyzing Weather Data with PySpark - Hottest Day Each Year\n","\n","Purpose: Identify the hottest day (highest MAX temperature) for each year across the cleaned data.\n","\n","Libraries Used:\n","\n","os: For file system operations.\n","pyspark.sql.SparkSession: For initializing a Spark session.\n","pyspark.sql.functions (aliased as F): For DataFrame operations like aggregation and ordering.\n","\n","\n","Process:\n","\n","Initialize a SparkSession with the app name \"WeatherAnalysis\".\n","Define the base path to the cleaned data (/content/cleaned_weather_data).\n","Create an empty dictionary hottest_days to store results.\n","Iterate over years 2015 to 2022:\n","Check if the year directory exists; skip if not.\n","For each CSV file in the year directory:\n","Load the CSV into a Spark DataFrame with spark.read.csv(), enabling header and schema inference.\n","Skip empty files using df.rdd.isEmpty().\n","Add a MAX column with None if missing using F.lit(None).\n","Compute the maximum MAX value using df.agg(F.max(\"MAX\")).\n","Filter the DataFrame for rows matching this maximum, order by DATE descending, and take the first row with first().\n","Store the station, name, date, and max temperature in hottest_days if not already present for that year.\n","\n","\n","\n","\n","If data is found, convert the results to a DataFrame and display it with show().\n","\n","\n","Output Example:\n","+----+-----------+--------------------+----------+----+\n","|YEAR|    STATION|                NAME|      DATE| MAX|\n","+----+-----------+--------------------+----------+----+\n","|2015|99495199999|SEBASTIAN INLET S...|2015-07-28|90.0|\n","|2016|72429793812|CINCINNATI MUNICI...|2016-07-26|93.9|\n","...\n","+----+-----------+--------------------+----------+----+\n","\n","\n","\n","\n","4. Analyzing Weather Data with PySpark - Coldest Day in March Across All Years\n","\n","Purpose: Identify the coldest day (lowest MIN temperature) in March across all years.\n","\n","Libraries Used: Same as the previous section.\n","\n","Process:\n","\n","Initialize a SparkSession with the app name \"Coldest Day in March\".\n","Define the base path to the cleaned data (/content/cleaned_weather_data).\n","Create an empty list march_data to store March-specific results.\n","Iterate over years 2015 to 2022:\n","Check if the year directory exists; skip if not.\n","For each CSV file in the year directory:\n","Load the CSV into a Spark DataFrame.\n","Skip empty files or files missing DATE or MIN columns.\n","Filter for March data using df.filter(df[\"DATE\"].contains('-03-')).\n","Order by MIN ascending and take the first row with first().\n","Append the station, name, date, and min temperature to march_data.\n","\n","\n","\n","\n","If March data is found:\n","Create a DataFrame from march_data.\n","Find the overall coldest day by ordering by MIN ascending and taking the first row.\n","Display the result.\n","\n","\n","\n","\n","Output Example:\n","Overall coldest day in March:\n","+-----------+--------------------+----------+---+\n","|    STATION|                NAME|      DATE|MIN|\n","+-----------+--------------------+----------+---+\n","|72429793812|CINCINNATI MUNICI...|2015-03-06|3.2|\n","+-----------+--------------------+----------+---+\n","\n","\n","\n","\n","PySpark Theory Concepts (Detailed Explanation)\n","1. SparkSession\n","\n","Definition: The entry point to PySpark's SQL and DataFrame API. It manages the Spark application and provides methods to create DataFrames, execute SQL queries, and configure Spark settings.\n","Details:\n","Created with SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate().\n","appName: Sets a name for the Spark application, visible in the Spark UI for tracking.\n","getOrCreate(): Reuses an existing session if available or creates a new one, ensuring resource efficiency.\n","Internally encapsulates a SparkContext, managing the connection to the Spark cluster and resource allocation.\n","\n","\n","\n","2. DataFrames\n","\n","Definition: A distributed collection of data organized into named columns, akin to a table in a relational database or a Pandas DataFrame, but optimized for distributed processing.\n","Details:\n","Created with spark.read.csv(file_path, header=True, inferSchema=True):\n","header=True: Uses the first row as column names.\n","inferSchema=True: Automatically detects column data types (e.g., integer, string).\n","\n","\n","Supports SQL-like operations (e.g., filtering, aggregation) executed across a cluster.\n","Built on top of RDDs but provides a higher-level abstraction for structured data.\n","\n","\n","\n","3. DataFrame Operations\n","\n","Filtering:\n","df.filter(df[\"DATE\"].contains('-03-')): Uses the contains method to match rows where DATE includes \"-03-\" (March).\n","Returns a new DataFrame with only the filtered rows.\n","\n","\n","Aggregation:\n","df.agg(F.max(\"MAX\")): Computes the maximum value in the MAX column.\n","Returns a DataFrame with one row and one column; .collect()[0][0] extracts the value.\n","\n","\n","Ordering:\n","df.orderBy(F.desc(\"DATE\")): Sorts the DataFrame by DATE in descending order.\n","F.asc(\"MIN\"): Sorts by MIN in ascending order.\n","Sorting is a transformation that prepares data for actions like first().\n","\n","\n","Collecting Data:\n","df.collect(): Retrieves all rows as a list of Row objects, triggering computation (action).\n","df.first(): Retrieves the first row as a Row object, also an action.\n","Row objects allow attribute access (e.g., row.STATION).\n","\n","\n","\n","4. Functions Module (pyspark.sql.functions)\n","\n","Definition: A library of built-in functions for manipulating DataFrame columns.\n","Details:\n","F.max(\"MAX\"): Returns the maximum value of the MAX column.\n","F.lit(None): Adds a column with a literal value (e.g., None) for missing columns.\n","F.asc(\"MIN\") and F.desc(\"DATE\"): Specify ascending or descending sort order.\n","Functions are lazily evaluated and optimized by Spark’s Catalyst optimizer.\n","\n","\n","\n","5. RDD (Resilient Distributed Dataset)\n","\n","Definition: The core data structure in Spark, representing an immutable, partitioned collection of objects that can be processed in parallel.\n","Details:\n","df.rdd.isEmpty(): Checks if the DataFrame’s underlying RDD has no data.\n","DataFrames are built on RDDs, but RDDs are lower-level and less structured.\n","Used here for emptiness checks, avoiding unnecessary processing.\n","\n","\n","\n","6. DataFrame Creation from Lists\n","\n","Definition: Allows creating a DataFrame from a Python list of tuples or Row objects with a specified schema.\n","Details:\n","spark.createDataFrame(hottest_days_list, [\"YEAR\", \"STATION\", \"NAME\", \"DATE\", \"MAX\"]):\n","Takes a list of tuples and a list of column names.\n","Distributes the data across the Spark cluster.\n","\n","\n","Used to present final results in a tabular format.\n","\n","\n","\n","7. Lazy Evaluation\n","\n","Definition: Spark delays execution of transformations until an action is called, optimizing the execution plan.\n","Details:\n","Transformations (e.g., filter, orderBy, agg) build a logical plan.\n","Actions (e.g., collect, first, show) trigger the execution of the plan.\n","Improves performance by combining operations and minimizing data shuffling.\n","\n","\n","\n","8. Actions vs. Transformations\n","\n","Transformations: Create a new DataFrame without immediate computation (e.g., filter, orderBy, withColumn).\n","Lazy, chainable, and optimized by Spark.\n","\n","\n","Actions: Trigger computation and return results to the driver (e.g., collect, first, show).\n","Examples: show() displays the DataFrame, first() retrieves a single row.\n","\n","\n","\n","9. Handling Missing Data\n","\n","Details:\n","Checks for missing columns (e.g., if \"MAX\" not in df.columns) and adds them with F.lit(None).\n","Skips files with missing required columns or empty data, ensuring robustness.\n","Prevents errors during aggregation or filtering.\n","\n","\n","\n","10. Directory and File Handling with PySpark\n","\n","Details:\n","os.path.join: Constructs platform-independent file paths.\n","os.listdir: Lists CSV files in a directory for processing.\n","Integrates seamlessly with PySpark’s file reading capabilities.\n","\n","\n","\n","\n","Summary\n","This notebook demonstrates a complete weather data analysis pipeline:\n","\n","Data Acquisition: Downloads CSV files from NOAA using requests.\n","Data Cleaning: Filters out invalid data using pandas.\n","Data Analysis: Uses PySpark to:\n","Find the hottest day each year.\n","Identify the coldest day in March across all years.\n","\n","\n","\n","Key PySpark concepts include:\n","\n","SparkSession: Central to managing Spark applications.\n","DataFrames: Efficient for structured data processing.\n","Transformations and Actions: Core to Spark’s lazy evaluation model.\n","Functions Module: Simplifies complex operations.\n","Error Handling: Ensures robustness with checks for empty files and missing columns.\n","\n","This explanation equips you with a thorough understanding of the code and PySpark principles for weather data analysis.'''\n"],"metadata":{"id":"uEAIOk-4YBnx"},"execution_count":null,"outputs":[]}]}